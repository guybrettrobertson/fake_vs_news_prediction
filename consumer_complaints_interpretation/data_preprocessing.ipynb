{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 100\n",
    "complaints_df = pd.read_csv('~/documents/data/consumer_complaints/consumer_complaints_clean.csv', \\\n",
    "                            index_col = 0, nrows = n_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make lower case\n",
    "\n",
    "Words with and without capital letters are considered to be different words by the nltk package. I can simplify the problem by making all letters lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in complaints_df.columns:\n",
    "    complaints_df[col] = [element.lower() for element in complaints_df[col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize, stem and remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I tokenize each of the complaints by splitting them into a list of separate words. I throw away punctuation at this point. This will make the remaining steps easier.\n",
    "\n",
    "I also stem each word to its root to save space, speed up the following analysis, and minimise any kind of overfitting to non-meaningful words.\n",
    "\n",
    "Stop words are commonly occuring words that take up space and add little meaning. Here I remove the stop words to reduce the size of the problem even further. \"xxxx\" is a a string used to replace words used in the consumer complaints with confidentiality issues. This string occurs frequently but as its meaning is obscured it adds no value. So here I add it to the list of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.append(\"xxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"\\w+\"\n",
    "complaints_list = []\n",
    "num_top_words = 50\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i in range(n_rows):\n",
    "    complaint = regexp_tokenize(complaints_df.iloc[i, 2], pattern)\n",
    "    complaints_list.append([ps.stem(word) for word in complaint if word not in stop_words])\n",
    "\n",
    "complaints_df['Consumer complaint narrative'] = complaints_list\n",
    "\n",
    "del complaints_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complaint = [word[0] for word in Counter(complaint).most_common(5)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
