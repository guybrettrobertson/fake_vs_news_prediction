{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whatsapp chat bot\n",
    "\n",
    "## Summary\n",
    "\n",
    "WhatsApp allows you to download the message history of any conversation. In this project, I have built a model that can take any WhatsApp message history and turn it into a chat bot. As the chat bot is based on the messages of the friend the user has been messaging, its 'personality' is based on that of the friend.\n",
    "\n",
    "The user of the chat bot uploads a WhatsApp message history with another person, and the chat bot learns from this dataset. The user then inputs a message, and the model finds a message sent by the user in the dataset which is most similar to the input message. The model then selects the response corresponding to this most similar message from the dataset. The response is then given by the model.\n",
    "\n",
    "## Chat bot methodology\n",
    "\n",
    "First, the model loads and processes the conversation history, which has been downloaded by the user directly from WhatsApp as a .txt file. Conversations tend to have groups of messages from each person before the other person responds. As a simplification, the model extracts the last message sent by the user, and the first message received in response to this. This gives a pair of messages: a message sent by the user and a response from the user's friend. So at this stage the model has two arrays of messages and corresponding responses.\n",
    "\n",
    "In processing the data, the model has an option to remove stop words. Stop words are typically removed from data used for Natural Language Processing because they are very common and have little specialised meaning. However, I have found that removing stop words in the chat bot worsens the results. This is likely to be because small talk contains a lot of stop words. For example, 'how', 'are', and 'you' are all stop words. By default, the chat bot therefore does not remove stop words.\n",
    "\n",
    "The chat bot also makes all of the chat data lower case, and by doing so assumes there is no important meaning in the distinction between upper and lower case letters in the WhatsApp conversation.\n",
    "\n",
    "Once the chat history has been converted into a list of messages from the user and responses to the user. The messages from the user are tagged and used to train a doc2vec model which uses a DM-PV (Distributed Memory version of Paragraph Vector) technique to convert each message into a numerical vector, where each vector corresponds to a position in 'meaning space'.\n",
    "\n",
    "After the chat bot has been trained on the input message history, it can have a conversation with the user. The user must input a message, and the chat bot then takes that message and converts it into a vector. The chat bot then finds the vector in the meaning space which closest to the new message vector. In other words, it finds the message that has the closest meaning to the new message. The chat bot then simply returns the response from the user's friend that corresponds to the closest message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat bot class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    \n",
    "    def __init__(self, my_name, friend_name, chat_file_path, remove_stop_words=False):\n",
    "        '''\n",
    "        Initialise the chat bot, and load and process the data.\n",
    "        '''\n",
    "        \n",
    "        # The name of the person who has downloaded the chat data\n",
    "        self.my_name = my_name\n",
    "        # The name of the other person\n",
    "        self.friend_name = friend_name\n",
    "        # Option to remove stop words from the data\n",
    "        self.remove_stop_words = remove_stop_words\n",
    "        \n",
    "        # Load the raw chat data\n",
    "        chat = open(chat_file_path)\n",
    "        chat_text = chat.read()\n",
    "        raw_chat = chat_text.splitlines()\n",
    "        \n",
    "        # List of messages from me\n",
    "        self.me_chat = []\n",
    "        # Corresponding list of responses to my messages\n",
    "        self.friend_chat = []\n",
    "        \n",
    "        previous_row = [None, None]\n",
    "\n",
    "        # Iterate through all messages\n",
    "        n = len(raw_chat)\n",
    "        for i in range(n):\n",
    "            row = raw_chat[i]\n",
    "            # Check that the row is not empty\n",
    "            if len(row) != 0:\n",
    "                # Check that the row is valid, excludes picture messages, etc.\n",
    "                if row[0] == '[':\n",
    "                    # Remove time stamp\n",
    "                    row = row.split('] ')[1]\n",
    "                    # Specifies who sent the message\n",
    "                    row = row.split(': ')[0:2]\n",
    "                    # Only keep last message in a string of message from me, and my friend's first response to this\n",
    "                    if previous_row[0] == my_name and row[0] != my_name:\n",
    "                        self.me_chat.append(previous_row[1])\n",
    "                        self.friend_chat.append(row[1])\n",
    "                    previous_row = row\n",
    "        \n",
    "        if self.remove_stop_words == True:\n",
    "            stop_words = stopwords.words('english')\n",
    "        \n",
    "        self.tagged_data = []\n",
    "        \n",
    "        # Create tagged documents for each message from me\n",
    "        n = len(self.me_chat)\n",
    "        for i in range(n):\n",
    "            message = word_tokenize(self.me_chat[i].lower())\n",
    "            if self.remove_stop_words == True:\n",
    "                tagged_message = [word for word in message if word not in stop_words]\n",
    "            else:\n",
    "                tagged_message = message\n",
    "            self.tagged_data.append(TaggedDocument(words=tagged_message, tags = [i]))\n",
    "\n",
    "    def train(self, max_epochs=100, vec_size=100, alpha=0.025):\n",
    "        \n",
    "        self.model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "        \n",
    "        self.model.build_vocab(self.tagged_data)\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            if epoch % 10 == 0: print('Iteration: ' +  str(epoch) + ' / ' + str(max_epochs))\n",
    "            self.model.train(self.tagged_data,\n",
    "                        total_examples=self.model.corpus_count,\n",
    "                        epochs=self.model.epochs)\n",
    "            # decrease the learning rate\n",
    "            self.model.alpha -= 0.0002\n",
    "            # fix the learning rate, no decay\n",
    "            self.model.min_alpha = self.model.alpha\n",
    "        \n",
    "        model_file_name = self.friend_name + \"_doc2vec.model\"\n",
    "        self.model.save(model_file_name)\n",
    "        self.model = Doc2Vec.load(model_file_name)\n",
    "        self.model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "        print('Iteration: ' +  str(max_epochs) + ' / ' + str(max_epochs))\n",
    "        print(\"Model Saved\")\n",
    "        \n",
    "    def message(self, message):\n",
    "        \n",
    "        message = word_tokenize(message.lower())\n",
    "        if self.remove_stop_words == True:\n",
    "            message = [word for word in message if word not in stop_words]\n",
    "        message_vector = self.model.infer_vector(message, epochs=1000)\n",
    "        similar_doc = self.model.docvecs.most_similar([message_vector])[0][0]\n",
    "        print(self.friend_name + ': ' + self.friend_chat[similar_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_bot = ChatBot(my_name='Guy', friend_name='Alison',\n",
    "                   chat_file_path='/Users/guybrett-robertson/Documents/data/whatsapp_chats/alison_chat.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 / 100\n"
     ]
    }
   ],
   "source": [
    "chat_bot.train(max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_bot.message('Ca va?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_bot.message('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_bot.message(\"How's it going?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
